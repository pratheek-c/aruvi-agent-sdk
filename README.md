# ğŸ§  Generic Multi-Provider AI Agent Framework

A lightweight, provider-agnostic AI Agent framework built in TypeScript.

- âœ… No SDK dependencies
- âœ… Raw HTTP only (fetch)
- âœ… Multi-provider support
- âœ… Tool calling system
- âœ… Streaming support
- âœ… Multi-agent routing
- âœ… Hono-based runtime
- âœ… Bun native compatible
- âœ… Vendor-neutral architecture

---
Supports:

- OpenAI
- Azure OpenAI
- Ollama (local)
- Claude (Anthropic)
- Easily extensible to other providers

---

# ğŸš€ Architecture

```
src/
â”œâ”€â”€ core/
â”‚     agent.ts
â”‚     types.ts
â”‚     llm.ts
â”‚
â”œâ”€â”€ providers/
â”‚     openai.provider.ts
â”‚     azure.provider.ts
â”‚     ollama.provider.ts
â”‚     claude.provider.ts
â”‚     openrouter.provider.ts
â”‚
â”œâ”€â”€ runtime/
â”‚     hono-adapter.ts
â”‚     multi-agent-runtime.ts

````

Design Philosophy:

- Agent is provider-agnostic
- Providers implement a shared interface
- Runtime is separated from core logic
- Tools are pluggable
- No SDK lock-in
- Streaming-first design

---

# ğŸ“¦ Installation

```bash
bun install
# or
npm install
````

No external SDKs required.

---

---

# ğŸ”Œ Supported Providers

| Provider     | SDK Used | Raw HTTP | Streaming |
| ------------ | -------- | -------- | --------- |
| OpenAI       | âŒ        | âœ…        | âœ…         |
| Azure OpenAI | âŒ        | âœ…        | Optional  |
| Ollama       | âŒ        | âœ…        | âœ…         |
| Claude       | âŒ        | âœ…        | Optional  |
| OpenRouter   | âŒ        | âœ…        | âœ…         |

---

## 1ï¸âƒ£ OpenAI

```ts
import { OpenAIProvider } from "./providers/openai.provider";

const provider = new OpenAIProvider(
  process.env.OPENAI_API_KEY!,
  "gpt-4o-mini"
);
```

Uses:

```
https://api.openai.com/v1/chat/completions
```

---

## 2ï¸âƒ£ Azure OpenAI

```ts
import { AzureProvider } from "./providers/azure.provider";

const provider = new AzureProvider(
  process.env.AZURE_API_KEY!,
  "https://your-resource.openai.azure.com",
  "gpt-4-deployment",
  "2024-02-15-preview"
);
```

---

## 3ï¸âƒ£ Ollama (Local)

```ts
import { OllamaProvider } from "./providers/ollama.provider";

const provider = new OllamaProvider("llama3");
```

Requires:

```
http://localhost:11434
```

---

## 4ï¸âƒ£ Claude (Anthropic)

```ts
import { ClaudeProvider } from "./providers/claude.provider";

const provider = new ClaudeProvider(
  process.env.CLAUDE_API_KEY!,
  "claude-3-sonnet-20240229"
);
```

---

# ğŸ§  Creating an Agent

```ts
import { Agent } from "./core/agent";

const agent = new Agent(provider, {
  systemPrompt: "You are a helpful AI assistant.",
  maxToolIterations: 3
});
```

---

# ğŸ›  Registering Tools

```ts
agent.registerTool({
  name: "calculator",
  description: "Evaluate math expression",
  async execute({ expression }) {
    return eval(expression).toString();
  }
});
```

---

# â–¶ï¸ Running the Agent

```ts
const result = await agent.run("What is 12 * 8?");
console.log(result);
```

---

# ğŸ”„ Tool Calling Flow

1. Agent sends system prompt + tool descriptions
2. LLM decides whether tool is required
3. If tool needed â†’ LLM responds with JSON:

```json
{
  "tool": "calculator",
  "arguments": { "expression": "12 * 8" }
}
```

4. Agent executes tool
5. Tool result is sent back to LLM
6. LLM produces final answer

Max iterations are configurable to prevent infinite loops.

---

# ğŸ§± Core Interfaces

## LLMProvider

```ts
export interface LLMProvider {
  name: string;
  chat(messages: Message[]): Promise<string>;
  stream?(messages: Message[]): AsyncGenerator<string>;
}
```

Streaming is optional per provider.

---

## Tool

```ts
export interface Tool {
  name: string;
  description: string;
  execute(args: any): Promise<string>;
}
```

---

# ğŸŒ Adding a New Provider

To add a new LLM:

1. Create a new provider file
2. Implement `LLMProvider`
3. Use raw HTTP `fetch`
4. Normalize response to return `string`

Example:

```ts
export class CustomProvider implements LLMProvider {
  name = "custom";

  async chat(messages: Message[]): Promise<string> {
    const res = await fetch("API_URL", {...});
    const data = await res.json();
    return data.response;
  }
}
```

Done.

No changes required in Agent.

---

# ğŸ” Environment Variables

Example `.env`:

```
OPENAI_API_KEY=...
AZURE_API_KEY=...
CLAUDE_API_KEY=...
```

---

# ğŸ§© Extending the Framework

Possible upgrades:

* Streaming abstraction
* Memory adapters (Redis / DB)
* Middleware pipeline
* Observability hooks
* Retry logic
* Tool schema validation (Zod)
* Multi-agent orchestration
* Rate limiting layer
* Logging & tracing
* OpenTelemetry integration

---

# ğŸ— Why No SDK?

Using raw HTTP:

* Reduces dependency surface
* Avoids breaking SDK changes
* Full control over request structure
* Works in Bun, Node, Edge, Deno
* Easier debugging
* Cleaner abstraction layer
---

# ğŸŒ API Endpoints

## Health Check

```
GET /agent/:name/health
```

Example:

```
GET /agent/router/health
```

---

## Standard Chat

```
POST /agent/:name/chat
```

Body:

```json
{
  "message": "What is 12 * 9?"
}
```

---

## ğŸ”¥ Streaming Chat

```
POST /agent/:name/stream
```

Returns streamed response (chunked).

Works with providers implementing `stream()`.

---

# ğŸŸ¢ Bun Native Runtime

No `@hono/node-server` required.

```ts
export default {
  port: 3000,
  fetch: app.fetch,
};
```

Run:

```bash
bun run index.ts
```

---
# ğŸ— Extending the Framework

Possible enhancements:

* SSE streaming parser
* Provider failover
* Middleware pipeline
* Redis memory adapter
* Agent orchestration engine
* Tool schema validation (Zod)
* Observability hooks
* Cost tracking
* OpenTelemetry support

---

# ğŸ¤ Contributing

1. Fork
2. Create feature branch
3. Add provider or feature
4. Submit PR

---

# ğŸ’¡ Philosophy

This project treats LLMs as infrastructure.

Not SDK-bound utilities.

It provides:

* Clean separation
* Vendor neutrality
* Pluggable architecture
* Production-grade runtime
* Multi-agent support
* Streaming-first design

---

# ğŸ§  Why No SDK?

Using raw HTTP:

* Reduces dependency surface
* Avoids breaking SDK updates
* Works in Bun, Node, Edge, Deno
* Easier debugging
* Fully controlled abstraction layer

---

Built for serious AI infrastructure development.

---
